{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DQN Agent\n",
    "\n",
    "In cases where the number of possible states becomes very large it becomes computationally inefficient to store the Q Values for each state in a table.\n",
    "\n",
    "To overcome this we use a Neural Network as a function approximator. We pass the input state and action to the network and retrieve the approximated Q Value.\n",
    "\n",
    "Since Q Learning is an Off policy algorithm it is able to learn from old experiences. We create a simple buffer using a deque that stores the last 200 frames. At each timestep we sample from the buffer for training.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import time\n",
    "from IPython.display import clear_output\n",
    "from matplotlib import pyplot as plt\n",
    "import random\n",
    "import gym\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from collections import deque\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize random agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Times steps survivied when using a Random Agent:  16.0\n"
     ]
    }
   ],
   "source": [
    "# Please note to render the Cart Pole Environment on a remote server you need to run the jupyter notebook using a virtual display.\n",
    "# Use the following command\n",
    "# xvfb-run -a -s \"-screen 0 1400x900x24 +extension RANDR\" -- jupyter notebook --no-browser\n",
    "\n",
    "env = gym.make(\"CartPole-v1\")\n",
    "\n",
    "state = env.reset()\n",
    "done = False\n",
    "total_reward = 0\n",
    "while not done:\n",
    "    clear_output(wait = True)\n",
    "    plt.imshow(env.render(mode='rgb_array'))\n",
    "    plt.show()\n",
    "    action = env.action_space.sample()\n",
    "    state, reward, done, info = env.step(action)\n",
    "    time.sleep(.1)\n",
    "    total_reward += reward\n",
    "print(\"Times steps survivied when using a Random Agent: \", total_reward)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Building the Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "EPISODES = 50\n",
    "\n",
    "class DQAgent:\n",
    "    def __init__(self, state_size, action_size):\n",
    "        self.state_size = state_size\n",
    "        self.action_size = action_size\n",
    "        self.memory = deque(maxlen=2000)\n",
    "        self.gamma = .95\n",
    "        self.epsilon = 1.0\n",
    "        self.epsilon_min = 0.01\n",
    "        self.epsilon_decay = 0.999\n",
    "        self.learning_rate = 0.001\n",
    "        \n",
    "        with tf.variable_scope(\"DQNetwork\"):\n",
    "            self.inputs_ = tf.placeholder(tf.float32, \n",
    "                                         [None, state_size], \n",
    "                                         name=\"inputs\")\n",
    "            self.actions_ = tf.placeholder(tf.float32,\n",
    "                                          action_size,\n",
    "                                          name=\"actions\")\n",
    "            self.target_Q = tf.placeholder(tf.float32,\n",
    "                                          (),\n",
    "                                          name=\"target_q\")\n",
    "            \n",
    "            self.fc1 = tf.layers.dense(inputs=self.inputs_,\n",
    "                                      units=24,\n",
    "                                      activation=tf.nn.elu,\n",
    "                                      kernel_initializer=tf.contrib.layers.xavier_initializer(),\n",
    "                                      name=\"fc1\"\n",
    "                                      )\n",
    "            \n",
    "            self.fc2 = tf.layers.dense(inputs=self.fc1,\n",
    "                                      units=24,\n",
    "                                      activation=tf.nn.elu,\n",
    "                                      kernel_initializer=tf.contrib.layers.xavier_initializer(),\n",
    "                                      name=\"fc2\")\n",
    "            \n",
    "            self.output = tf.layers.dense(inputs=self.fc2,\n",
    "                                         units = action_size,\n",
    "                                         activation=None,\n",
    "                                         kernel_initializer=tf.contrib.layers.xavier_initializer(),\n",
    "                                         name=\"output\")\n",
    "            self.Q = tf.reduce_sum(tf.multiply(self.output, self.actions_))\n",
    "            self.loss = tf.reduce_mean(tf.square(self.target_Q - self.Q))\n",
    "            \n",
    "            self.optimizer = tf.train.AdamOptimizer(self.learning_rate).minimize(self.loss)\n",
    "            \n",
    "    def remember(self, state, action, reward, next_state, done):\n",
    "        self.memory.append((state, action, reward, next_state, done))\n",
    "\n",
    "    def act(self, state, sess):\n",
    "        if np.random.rand() <= self.epsilon:\n",
    "            return random.randrange(self.action_size)\n",
    "        else:\n",
    "            act_values = sess.run(self.output, feed_dict={\n",
    "                self.inputs_ : state\n",
    "            })\n",
    "            return np.argmax(act_values)\n",
    "\n",
    "    def replay(self, batch_size, sess):\n",
    "        minibatch = random.sample(self.memory, batch_size)\n",
    "        for state, action, reward, next_state, done in minibatch:\n",
    "            target = reward\n",
    "            if not done:\n",
    "                target_q = sess.run(self.output, feed_dict={\n",
    "                    self.inputs_ : next_state\n",
    "                })\n",
    "                target = (reward + self.gamma * np.amax(target_q[0]))\n",
    "            #Remove incorrect indent, we want to train at each timestep not just when it is done\n",
    "            action = np.eye(self.action_size)[action]\n",
    "\n",
    "            loss, _ = sess.run([self.loss, self.optimizer], feed_dict={\n",
    "                self.inputs_ : state,\n",
    "                self.target_Q : target,\n",
    "                self.actions_ : action\n",
    "            })\n",
    "\n",
    "\n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode 0, score: 12, e: 1.0\n",
      "episode 1, score: 16, e: 1.0\n",
      "episode 2, score: 14, e: 0.9870777147137147\n",
      "episode 3, score: 16, e: 0.9704309672630859\n",
      "episode 4, score: 10, e: 0.959809440525076\n",
      "episode 5, score: 11, e: 0.9483548639781193\n",
      "episode 6, score: 26, e: 0.9230793978373364\n",
      "episode 7, score: 11, e: 0.9120631656822724\n",
      "episode 8, score: 19, e: 0.8939941590229386\n",
      "episode 9, score: 20, e: 0.8754068367770318\n",
      "episode 10, score: 62, e: 0.8219316276865206\n",
      "episode 11, score: 25, e: 0.8008264083574369\n",
      "episode 12, score: 13, e: 0.7896874231428072\n",
      "episode 13, score: 21, e: 0.7724955072656065\n",
      "episode 14, score: 14, e: 0.7609888362515699\n",
      "episode 15, score: 13, e: 0.750403966288439\n",
      "episode 16, score: 59, e: 0.7066826263699144\n",
      "episode 17, score: 30, e: 0.6851009179515178\n",
      "episode 18, score: 51, e: 0.6503691570122084\n",
      "episode 19, score: 27, e: 0.632402542800493\n",
      "episode 20, score: 76, e: 0.5855125400671733\n",
      "episode 21, score: 95, e: 0.531891525167934\n",
      "episode 22, score: 109, e: 0.47646036099556505\n",
      "episode 23, score: 36, e: 0.45914497940338683\n",
      "episode 24, score: 54, e: 0.4345619451717332\n",
      "episode 25, score: 108, e: 0.3896636631051653\n",
      "episode 26, score: 153, e: 0.3340220181386073\n",
      "episode 27, score: 175, e: 0.2800922055970794\n",
      "episode 28, score: 199, e: 0.22929715625813843\n",
      "episode 29, score: 197, e: 0.18808984016399252\n",
      "episode 30, score: 199, e: 0.15397952748707622\n",
      "episode 31, score: 199, e: 0.12605515994096825\n",
      "episode 32, score: 199, e: 0.10319490913541582\n",
      "episode 33, score: 199, e: 0.08448039157186232\n",
      "episode 34, score: 199, e: 0.06915977367420187\n",
      "episode 35, score: 199, e: 0.05661756776539281\n",
      "episode 36, score: 199, e: 0.046349905579066174\n",
      "episode 37, score: 199, e: 0.037944295948747826\n",
      "episode 38, score: 199, e: 0.03106305346383333\n",
      "episode 39, score: 199, e: 0.02542973235819944\n",
      "episode 40, score: 199, e: 0.020818020628994943\n",
      "episode 41, score: 199, e: 0.017042648219988785\n",
      "episode 42, score: 199, e: 0.013951944016509982\n",
      "episode 43, score: 199, e: 0.011421742638067355\n",
      "episode 44, score: 199, e: 0.009998671593271896\n",
      "episode 45, score: 199, e: 0.009998671593271896\n",
      "episode 46, score: 199, e: 0.009998671593271896\n",
      "episode 47, score: 199, e: 0.009998671593271896\n",
      "episode 48, score: 199, e: 0.009998671593271896\n",
      "episode 49, score: 199, e: 0.009998671593271896\n"
     ]
    }
   ],
   "source": [
    "env = gym.make(\"CartPole-v1\")\n",
    "state_size = env.observation_space.shape[0]\n",
    "action_size = env.action_space.n\n",
    "\n",
    "tf.reset_default_graph()\n",
    "agent = DQAgent(state_size, action_size)\n",
    "\n",
    "sess = tf.Session()\n",
    "sess.run(tf.global_variables_initializer())\n",
    "done = False\n",
    "batch_size = 32\n",
    "\n",
    "for e in range(EPISODES):\n",
    "    state = env.reset()\n",
    "    state = np.reshape(state, (1, state_size))\n",
    "    for time in range(200):\n",
    "        action = agent.act(state, sess)\n",
    "        next_state, reward, done, _ = env.step(action)\n",
    "        next_state = np.reshape(next_state, (1, state_size))\n",
    "        reward = reward if not done else -10.0\n",
    "        agent.remember(state, action, reward, next_state, done)\n",
    "        state = next_state\n",
    "\n",
    "        if len(agent.memory)> batch_size:\n",
    "            agent.replay(batch_size, sess)\n",
    "            if agent.epsilon > agent.epsilon_min:\n",
    "                agent.epsilon *= agent.epsilon_decay\n",
    "        if done:\n",
    "            break\n",
    "    print(f\"episode {e}, score: {time}, e: {agent.epsilon}\")\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluating Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results after 100 episodes:\n",
      "Average Reward per episode: 290.11\n"
     ]
    }
   ],
   "source": [
    "agent.epsilon = 0.0\n",
    "episodes = 100\n",
    "\n",
    "total_reward = 0\n",
    "for _ in range(episodes):\n",
    "    state = env.reset()\n",
    "    done = False\n",
    "    while not done:\n",
    "        state = np.reshape(state, (1, state_size))\n",
    "        action = agent.act(state, sess)\n",
    "        state, reward, done, _ = env.step(action)\n",
    "        total_reward += reward\n",
    "\n",
    "print(f\"Results after {episodes} episodes:\")\n",
    "print(f\"Average Reward per episode: {total_reward / episodes}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "sess.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
