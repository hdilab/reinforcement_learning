{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Actor Critic Methods\n",
    "\n",
    "Actor Critic Methods combine both Value Based and Policy Gradient Methods.\n",
    "\n",
    "##### Value Based Methods:\n",
    "- Maps each state action pair to a value.\n",
    "- Take action with highest value when exploiting.\n",
    "- Works for finite set of actions.\n",
    "- eg Q Learning, DQN\n",
    "\n",
    "$\n",
    "\\begin{align}\n",
    "Q(s, a) = Q(s, a) + \\alpha [r + \\gamma max_{a^{'}}Q(s^{'}, a^{'}) - Q(s, a)]\n",
    "\\end{align}\n",
    "$\n",
    "\n",
    "\n",
    "\n",
    "##### Policy Gradient Methods:\n",
    "- Directly optimize the policy given a state.\n",
    "- Maps state to action distribution.\n",
    "- Useful when large number of possible actions.\n",
    "- eg Policy Gradients\n",
    "\n",
    "\n",
    "$\n",
    "\\begin{align}\n",
    "\\bigtriangledown \\text{J}(\\theta) = E_{\\pi}[ \\bigtriangledown \\log(\\pi(\\tau)) r(\\tau) ]\n",
    "\\end{align}\n",
    "$\n",
    "\n",
    "\n",
    "Issues with Policy Gradients\n",
    "- High variance due to different trajectories being taken from the same step in different episodes.\n",
    "- No Learning takes place when cumulative reward is 0\n",
    "- Monte Carlo Method so have to wait until end of trajectory to process.\n",
    "\n",
    "Instead of using the discounted future reward of the trajectory we can use an estimate of the Q Value.\n",
    "This can be done by running two networks in parallel. One to learn the Q value and the second that uses policy gradients to learn the action probability distribution.\n",
    "\n",
    "$\n",
    "\\begin{align}\n",
    "\\bigtriangledown \\text{J}(\\theta) = E_{\\pi}[ \\bigtriangledown \\log(\\pi(\\tau)) Q(s_{t}, a_{t}) ]\n",
    "\\end{align}\n",
    "$\n",
    "\n",
    "Since we can plug in the Q Value we reduce the variance as well as allow us to use a temporal difference method instead of Monte Carlo.\n",
    "\n",
    "Using Advantage\n",
    "$\n",
    "\\begin{align}\n",
    "A(s_{t}, a_{t}) = Q(s_{t}, a_{t}) - V(s_{t})\n",
    "\\end{align}\n",
    "$\n",
    "\n",
    "$\n",
    "\\begin{align}\n",
    "A(s_{t}, a_{t}) = r_{t+1} + \\gamma V(s_{t+1}) - V(s_{t})\n",
    "\\end{align}\n",
    "$\n",
    "\n",
    "\n",
    "$\n",
    "\\begin{align}\n",
    "\\bigtriangledown \\text{J}(\\theta) = E_{\\pi}[ \\bigtriangledown \\log(\\pi(\\tau)) A(s_{t}, a_{t}) ]\n",
    "\\end{align}\n",
    "$\n",
    "\n",
    "Here we take the Q value for a state minus the average Value for the state. We can think of it as how much better or worse the current action is compared with the rest of the actions.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "import sys"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Building the Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ActorCritic:\n",
    "    def __init__(self, sess):\n",
    "        self.sess = sess\n",
    "        with tf.variable_scope(\"a2c\", reuse=tf.AUTO_REUSE):\n",
    "            self.inputs = tf.placeholder(tf.float32, [None, 4], name=\"inputs\")\n",
    "            \n",
    "            self.fc1 = tf.layers.dense(inputs = self.inputs,\n",
    "                                      units = 64,\n",
    "                                      activation = tf.nn.relu,\n",
    "                                      kernel_initializer=tf.contrib.layers.xavier_initializer())\n",
    "            self.fc2 = tf.layers.dense(inputs = self.fc1,\n",
    "                                      units = 128,\n",
    "                                      activation = tf.nn.relu,\n",
    "                                      kernel_initializer=tf.contrib.layers.xavier_initializer())\n",
    "            # Instead of having two different networks we can also share the first few layers for both the Actor and Critic.\n",
    "            # The layer fc3 will be used as the input to the final layers for the actor and critic.\n",
    "            self.fc3 = tf.layers.dense(inputs = self.fc2,\n",
    "                                      units = 64,\n",
    "                                      activation = tf.nn.relu,\n",
    "                                      kernel_initializer=tf.contrib.layers.xavier_initializer())\n",
    "            # Actor\n",
    "            # Return the Probability Distribution for each action.\n",
    "            self.action_prob = tf.layers.dense(inputs = self.fc3,\n",
    "                                              units = 2,\n",
    "                                              activation = tf.nn.softmax,\n",
    "                                              kernel_initializer=tf.contrib.layers.xavier_initializer())\n",
    "            \n",
    "            #Critic\n",
    "            # Single node to output the Value Function\n",
    "            self.state_value = tf.layers.dense(inputs = self.fc3,\n",
    "                                              units = 1,\n",
    "                                              activation = None,\n",
    "                                              kernel_initializer=tf.contrib.layers.xavier_initializer())\n",
    "            \n",
    "            self.rewards = tf.placeholder(tf.float32, [None, 1], name=\"rewards\")\n",
    "            self.actions = tf.placeholder(tf.float32, [None, 2], name=\"actions\")\n",
    "            self.state_values_est = tf.placeholder(tf.float32, [None, 1], name=\"value_estimates\")\n",
    "            \n",
    "            #  Calculate the Log of the probabilities.\n",
    "            self.action_log_probs = tf.log(self.action_prob)\n",
    "            \n",
    "            \n",
    "            self.chosen_action_log_probs = tf.multiply(self.actions, self.action_log_probs)\n",
    "            \n",
    "            # Calculate TD Error\n",
    "            self.advantages = self.rewards - self.state_values_est\n",
    "            self.value_loss = tf.reduce_mean(tf.pow(self.advantages, 2))\n",
    "            \n",
    "            # Add entropy to increase exploration,\n",
    "            self.entropy = tf.reduce_mean(tf.multiply(self.action_prob, self.action_log_probs))\n",
    "            self.action_gain = tf.reduce_mean(tf.multiply(self.chosen_action_log_probs, self.advantages))\n",
    "            \n",
    "            self.total_loss = self.value_loss - self.action_gain - 0.0001*self.entropy\n",
    "            self.optimizer = tf.train.AdamOptimizer(0.0001).minimize(self.total_loss)\n",
    "            \n",
    "    # Only Actor\n",
    "    def get_action(self, sess, state):\n",
    "        action_prob = sess.run([self.action_prob], {self.inputs:state})\n",
    "        action_prob = action_prob[0]\n",
    "        action = np.random.choice(action_prob.shape[1], action_prob.shape[0], p=action_prob[0].ravel())\n",
    "        action = np.asscalar(action)\n",
    "        return action\n",
    "    \n",
    "    # Only Critic\n",
    "    def get_state_value(self, sess, state):\n",
    "        value = sess.run([self.state_value], {self.inputs:state})\n",
    "        return value\n",
    "    \n",
    "    # Both Actor and Critic\n",
    "    def evaluate_actions(self, state):\n",
    "        action_prob, value = sess.run([self.action_prob, self.state_value], {self.inputs:state})\n",
    "        return action_prob, state_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "GAMMA = .95\n",
    "LEARNING_RATE = 0.01\n",
    "N_GAMES = 2000\n",
    "N_STEPS = 20\n",
    "env = gym.make('CartPole-v0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_discounted_rewards(model, rewards, dones):\n",
    "    \n",
    "    R = []\n",
    "    rewards.reverse()\n",
    "\n",
    "    # If we happen to end the set on a terminal state, set next return to zero\n",
    "    if dones[-1] == True: \n",
    "        next_return = 0\n",
    "        \n",
    "    # If not terminal state, bootstrap v(s) using our critic\n",
    "    # TODO: don't need to estimate again, just take from last value of v(s) estimates\n",
    "    else: \n",
    "        value = model.get_state_value(model.sess, states[-1])\n",
    "        next_return = value[0][0]\n",
    "    \n",
    "    # Backup from last state to calculate \"true\" returns for each state in the set\n",
    "    R.append(next_return)\n",
    "    dones.reverse()\n",
    "    for r in range(1, len(rewards)):\n",
    "        if not dones[r]: \n",
    "            this_return = rewards[r] + next_return * GAMMA\n",
    "        else:\n",
    "            this_return = 0\n",
    "        R.append(this_return)\n",
    "        next_return = this_return\n",
    "\n",
    "    R.reverse()\n",
    "    return R"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reflect(model, states, actions, rewards, dones):\n",
    "    states = np.reshape(states, [-1, 4])\n",
    "    discounted_rewards = get_discounted_rewards(model, rewards, dones)\n",
    "    discounted_rewards = np.reshape(discounted_rewards, [-1, 1])\n",
    "    actions = np.reshape(actions, [-1, 1])\n",
    "    actions = np.eye(2)[actions]\n",
    "    actions = np.reshape(actions, [-1, 2])\n",
    "    state_value_estimates = model.get_state_value(sess, states)\n",
    "    state_value_estimates = np.reshape(state_value_estimates, [-1, 1])\n",
    "    \n",
    "    sess.run([model.total_loss, model.optimizer], {\n",
    "        model.inputs: states,\n",
    "        model.rewards: discounted_rewards,\n",
    "        model.actions: actions,\n",
    "        model.state_values_est: state_value_estimates\n",
    "    })"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Games Finished 50 total_reward 18.0\n",
      "Games Finished 100 total_reward 20.0\n",
      "Games Finished 150 total_reward 74.0\n",
      "Games Finished 200 total_reward 18.0\n",
      "Games Finished 250 total_reward 12.0\n",
      "Games Finished 300 total_reward 45.0\n",
      "Games Finished 350 total_reward 39.0\n",
      "Games Finished 400 total_reward 60.0\n",
      "Games Finished 450 total_reward 69.0\n",
      "Games Finished 500 total_reward 18.0\n",
      "Games Finished 550 total_reward 20.0\n",
      "Games Finished 600 total_reward 35.0\n",
      "Games Finished 650 total_reward 36.0\n",
      "Games Finished 700 total_reward 34.0\n",
      "Games Finished 750 total_reward 119.0\n",
      "Games Finished 800 total_reward 55.0\n",
      "Games Finished 850 total_reward 86.0\n",
      "Games Finished 900 total_reward 51.0\n",
      "Games Finished 950 total_reward 97.0\n",
      "Games Finished 1000 total_reward 88.0\n",
      "Games Finished 1050 total_reward 18.0\n",
      "Games Finished 1100 total_reward 83.0\n",
      "Games Finished 1150 total_reward 45.0\n",
      "Games Finished 1200 total_reward 106.0\n",
      "Games Finished 1250 total_reward 102.0\n",
      "Games Finished 1300 total_reward 29.0\n",
      "Games Finished 1350 total_reward 200.0\n",
      "Games Finished 1400 total_reward 166.0\n",
      "Games Finished 1450 total_reward 101.0\n",
      "Games Finished 1500 total_reward 200.0\n",
      "Games Finished 1550 total_reward 200.0\n",
      "Games Finished 1600 total_reward 200.0\n",
      "Games Finished 1650 total_reward 200.0\n",
      "Games Finished 1700 total_reward 139.0\n",
      "Games Finished 1750 total_reward 180.0\n",
      "Games Finished 1800 total_reward 83.0\n",
      "Games Finished 1850 total_reward 127.0\n",
      "Games Finished 1900 total_reward 114.0\n",
      "Games Finished 1950 total_reward 30.0\n",
      "Games Finished 2000 total_reward 200.0\n"
     ]
    }
   ],
   "source": [
    "state = env.reset()\n",
    "finished_games = 0\n",
    "\n",
    "sess = tf.Session()\n",
    "model = ActorCritic(sess)\n",
    "sess.run(tf.global_variables_initializer())\n",
    "total_reward = 0\n",
    "state_size = env.observation_space.shape[0]\n",
    "while finished_games < N_GAMES:\n",
    "    states, actions, rewards, dones = [], [], [], []\n",
    "    # Gather training data\n",
    "    for i in range(N_STEPS):\n",
    "        state = np.reshape(state, [-1, state_size])\n",
    "        action = model.get_action(sess, state)\n",
    "\n",
    "        next_state, reward, done, _ = env.step(action)\n",
    "\n",
    "        states.append(state)\n",
    "        actions.append(action)\n",
    "        rewards.append(reward)\n",
    "        dones.append(done)\n",
    "        total_reward += reward\n",
    "\n",
    "        if done: \n",
    "            state = env.reset()\n",
    "            finished_games += 1\n",
    "            if finished_games % 50 == 0:\n",
    "                print(\"Games Finished\", finished_games, \"total_reward\", total_reward)\n",
    "            total_reward = 0\n",
    "        else:\n",
    "            state = next_state\n",
    "\n",
    "    # Reflect on training data\n",
    "    reflect(model, states, actions, rewards, dones)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate agents performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results after 100 episodes:\n",
      "Average Reward per episode: 177.18\n"
     ]
    }
   ],
   "source": [
    "episodes = 100\n",
    "\n",
    "total_reward = 0\n",
    "for _ in range(episodes):\n",
    "    state = env.reset()\n",
    "    done = False\n",
    "    while not done:\n",
    "        action_probability_distribution = sess.run(model.action_prob, feed_dict = {\n",
    "            model.inputs : state.reshape([-1, state_size])\n",
    "        })\n",
    "        action = np.random.choice(range(action_probability_distribution.shape[1]), p=action_probability_distribution.ravel())\n",
    "        state, reward, done, _ = env.step(action)\n",
    "        total_reward += reward\n",
    "\n",
    "print(f\"Results after {episodes} episodes:\")\n",
    "print(f\"Average Reward per episode: {total_reward / episodes}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "sess.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
